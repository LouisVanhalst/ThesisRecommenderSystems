{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f0cc3a-289b-43c1-9788-a1dde2a192a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Load YahooMusic Dataset\n",
    "u_nodes = np.loadtxt(\"../datasets/Douban/u_nodes_ratings.csv\")\n",
    "v_nodes = np.loadtxt(\"../datasets/Douban/v_nodes_ratings.csv\")\n",
    "ratings = np.loadtxt(\"../datasets/Douban/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8b2d97-3951-4815-92d2-e20bf8806401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the dataframe. Column names are irrelevant.\n",
    "ratings_dict = {'itemID': v_nodes,\n",
    "                'userID': u_nodes,\n",
    "                'rating': ratings}\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from surprise import NormalPredictor\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df[['userID', 'itemID', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148aa8dc-ac92-4494-a04e-62b9bcbb1fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.7387\n",
      "MAE:  0.5819\n",
      "fit-time: 41.2087037563324\n",
      "test-time: 1.6586456298828125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from surprise import SVDpp\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "import time\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# sample random trainset and testset\n",
    "# test set is made of 30% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.30)\n",
    "\n",
    "# We'll use the KNN algorithm.\n",
    "algo = SVDpp()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "start_fit = time.time()\n",
    "algo.fit(trainset)\n",
    "\n",
    "fit_time = time.time() - start_fit\n",
    "start_test = time.time()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "\n",
    "#print(predictions)\n",
    "test_time = time.time() - start_test\n",
    "# Compute the metrics\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)\n",
    "print(\"fit-time: \" + str(fit_time))\n",
    "print(\"test-time: \" + str(test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336761b3-4b3a-4fea-baaa-f747cf640b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9438023400069223"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get NDCG\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def get_ndcg(surprise_predictions, k_highest_scores=None):\n",
    "    \"\"\" \n",
    "    Calculates the ndcg (normalized discounted cumulative gain) from surprise predictions, using sklearn.metrics.ndcg_score and scipy.sparse\n",
    "  \n",
    "    Parameters: \n",
    "    surprise_predictions (List of surprise.prediction_algorithms.predictions.Prediction): list of predictions\n",
    "    k_highest_scores (positive integer): Only consider the highest k scores in the ranking. If None, use all. \n",
    "  \n",
    "    Returns: \n",
    "    float in [0., 1.]: The averaged NDCG scores over all recommendations\n",
    "  \n",
    "    \"\"\"\n",
    "    from sklearn.metrics import ndcg_score\n",
    "    from scipy import sparse\n",
    "    \n",
    "    uids = [int(p.uid) for p in surprise_predictions ]\n",
    "    iids = [int(p.iid) for p in surprise_predictions ]\n",
    "    r_uis = [p.r_ui for p in surprise_predictions ]\n",
    "    ests = [p.est for p in surprise_predictions ]\n",
    "    \n",
    "    assert(len(uids) == len(iids) == len(r_uis) == len(ests) )    \n",
    "    \n",
    "    sparse_preds = sparse.coo_matrix( (ests, (uids , iids )) )\n",
    "    sparse_vals = sparse.coo_matrix( (r_uis, (uids , iids )) )\n",
    "    \n",
    "    dense_preds = sparse_preds.toarray()\n",
    "    dense_vals = sparse_vals.toarray()\n",
    "    \n",
    "    return ndcg_score(y_true= dense_vals , y_score= dense_preds, k=k_highest_scores)\n",
    "\n",
    "get_ndcg(predictions, k_highest_scores = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf71a2-9298-4e5d-ae08-eaeda3ed7ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
